Xavier initialization for weights for neuroevolution (keras init)
see https://arxiv.org/pdf/1712.06567.pdf

temperature in addition or replacement to dirichlet noise (using stochastic moves up to specific depth)
see https://github.com/LeelaChessZero/lczero/blob/release/src/UCTSearch.cpp
hyperparameter tuning
use random seeds to distribute nodes and build client/server architecture
port to opencl DONE
add second command queue and more clever batching for GPU
node hashing DONE but little usage with so few playouts
virtual batchnorm -> prevent local minimum DONE
virtual visits for MCTS to OpenCL port DONE
what about loops in MCTS tree? should re-use stats
change cpuct value (its not a constant)
add FPU DONE
add weight regularization loss DONE
dont re-use the weights which were not accepted ?
http://blog.lczero.org/2018/12/alphazero-paper-and-lc0-v0191.html
implement a global hash for BP training
implement training window to re-use old data DONE
add plots for keras training loss 
speedup weight creations NOT SO SLOW
use localID and globalID to share weights on GPU DONE
should be possible to re-use tree during self play for opposite player
tensorboard
>python.exe -m tensorboard.main --logdir "Z:\CloudStation\GitHub Projects\TicTacToe-DL-RL\Training\log\"